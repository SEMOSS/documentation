"use strict";(self.webpackChunkcfg_docs=self.webpackChunkcfg_docs||[]).push([[9604],{50203:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var r=t(74848),i=t(28453);const a={sidebar_label:"LLM Hosting Options",sidebar_position:4},o="LLM Hosting Options",s={id:"Advanced Installation/Deploying a Private Server/LLM Hosting Options",title:"LLM Hosting Options",description:"vLLM: Fast and User-Friendly LLM Library",source:"@site/docs/Advanced Installation/Deploying a Private Server/LLM Hosting Options.md",sourceDirName:"Advanced Installation/Deploying a Private Server",slug:"/Advanced Installation/Deploying a Private Server/LLM Hosting Options",permalink:"/documentation/Advanced Installation/Deploying a Private Server/LLM Hosting Options",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_label:"LLM Hosting Options",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"AI Server Configuration Parameters",permalink:"/documentation/Advanced Installation/Deploying a Private Server/AI Server Configuration Parameters"},next:{title:"Deploying a React App",permalink:"/documentation/Advanced Installation/Deploying a Private Server/Deploying An App"}},l={},d=[{value:"vLLM: Fast and User-Friendly LLM Library",id:"vllm-fast-and-user-friendly-llm-library",level:2},{value:"Text Generation Inference (TGI)",id:"text-generation-inference-tgi",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",p:"p",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"llm-hosting-options",children:"LLM Hosting Options"}),"\n",(0,r.jsx)(n.h2,{id:"vllm-fast-and-user-friendly-llm-library",children:"vLLM: Fast and User-Friendly LLM Library"}),"\n",(0,r.jsxs)(n.p,{children:["vLLM is designed for rapid Large Language Model (LLM) inference and serving.\r\nFor comprehensive documentation and further details, please visit ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"here"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"text-generation-inference-tgi",children:"Text Generation Inference (TGI)"}),"\n",(0,r.jsxs)(n.p,{children:["Text Generation Inference (TGI) stands as a versatile toolkit crafted for the seamless deployment and efficient serving of Large Language Models (LLMs). By harnessing the power of TGI, users can unlock unparalleled performance in text generation. TGI is meticulously designed to cater to a wide array of the most renowned open-source LLMs, such as Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and beyond.\r\nFor further exploration and detailed documentation, visit ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/text-generation-inference/index",children:"here"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var r=t(96540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);